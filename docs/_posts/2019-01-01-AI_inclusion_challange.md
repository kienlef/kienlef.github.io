---
title: AI Inclusion / Selection Bias Challenge
tags: Ethics
---

Intelligence is the ability to learn from experience, solve problems, and use knowledge to adapt to new situations (David G. Myers, Psychology 12 Edition). I refer to artificial intelligence (AI) systems as a collection of advanced technologies that allow machines to sense, comprehend, act, and learn.

Machine learning and statistical models are often within the heart of these AI or data-driven systems.

Three major pitfalls I experience over and over again when designing statistical / machine learning models for data-driven systems.

* not knowing the business value and definition of good
* selecting wrong or biased information
* designing models which are too complex and costly to maintain

In the following, I will only focus on the selection bias problem.

Machine learning systems and as well humans drive the learning through iterations with data / information. The quality, amount, preparation, and selection of data are critical to the success of a machine learning solution.

One common statement in machine learning is garbage in will lead to garbage out. Of course, I fully agree with this statement. However, often it is not clear what garbage is.

There are obvious issues related to data quality problems, like missing data or outliers. Even before we can judge on the quality of data we have to select a representative data set which is linked to your business application.

Any AI engine and as well every statistic is based on the seen datasets.

However, every underlying dataset is the products of human decisions. Within the selection and curation of data, human biases occur which will show up in the outputs of AI systems.

An AI inclusion problem is summarized in this video. The video shows the biased output with respect to the search term ‚family‘.

From a society or ethical point of view it is for sure a large problem, often summarized as AI inclusion problem. Note, that the term AI inclusion many different aspects are typically discussed around development, social impact, policy implications, and legal issues concerning AI systems, see e.g. [aiandinclusion](https://aiandinclusion.org)

Technically speaking, what happens in this video is a selection bias problem.

Selection bias occurs when the samples used to produce the model are not fully representative of cases that the model may be used for in the future.

The selection bias comes not only within AI systems, it lies in the heart of any statistical evaluation.

In a data-driven world decisions will be based or even automated on statistical models. Thus, a basic understanding of statistics is mandatory to judge critically on results and outputs.

In my current data science lecture class, I realize a strong wish towards content featuring deep learning systems. However, we have to teach the basics first before going to fast into modeling. Within our data science education programs we have to focus more on statistics and thus the judgment of AI systems instead of teaching fancy algorithms.

At any work towards AI integration in an enterprise, we should established as well a critical thinking towards selection bias within a CRISP data science practice. Identifying selection bias should be as well one topic in any training program.

For me, understanding selection bias and to know how to measure and prevent it should be one central point while teaching data science or AI systems. Empowering more individuals with AI / statistics education will lead to critical and profound thinking which is essential within any decision-making process.

What do you think - teaching explicitly the topic selection bias - would it help to tackle the bigger AI inclusion problem?
